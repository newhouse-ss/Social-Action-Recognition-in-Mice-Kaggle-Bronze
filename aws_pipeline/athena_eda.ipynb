{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless EDA Pipeline — Amazon Athena Simulation\n",
    "\n",
    "This notebook simulates **Amazon Athena** ad-hoc queries against the\n",
    "**Curated Zone** Parquet files produced by the Glue ETL pipeline.\n",
    "\n",
    "Locally we use **PySpark + SparkSQL** as a drop-in replacement for Athena.\n",
    "In production, you would point Athena at the same S3 Curated Zone paths\n",
    "via the AWS Glue Data Catalog and run identical SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from aws_pipeline.config import LOCAL_CURATED_ZONE, SPARK_APP_NAME\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='muted', font_scale=1.1)\n",
    "plt.rcParams.update({'figure.dpi': 100})\n",
    "\n",
    "CURATED = LOCAL_CURATED_ZONE\n",
    "print(f'Curated Zone: {CURATED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Spin up a local SparkSession (simulates Athena engine) ──\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(f'{SPARK_APP_NAME}-EDA')\n",
    "    .master('local[*]')\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "print('SparkSession ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Register Curated Parquet as SQL Tables\n",
    "\n",
    "In production these would be **Glue Data Catalog tables** queryable by Athena.\n",
    "Here we register them as Spark temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "meta_train = spark.read.parquet(os.path.join(CURATED, 'metadata_train'))\n",
    "meta_train.createOrReplaceTempView('metadata_train')\n",
    "print(f'metadata_train: {meta_train.count()} rows, {len(meta_train.columns)} cols')\n",
    "\n",
    "meta_test = spark.read.parquet(os.path.join(CURATED, 'metadata_test'))\n",
    "meta_test.createOrReplaceTempView('metadata_test')\n",
    "print(f'metadata_test: {meta_test.count()} rows')\n",
    "\n",
    "# Enriched annotations\n",
    "anno = spark.read.parquet(os.path.join(CURATED, 'annotations_enriched'))\n",
    "anno.createOrReplaceTempView('annotations')\n",
    "print(f'annotations: {anno.count()} rows')\n",
    "\n",
    "# Class distribution stats\n",
    "stats = spark.read.parquet(os.path.join(CURATED, 'class_distribution_stats'))\n",
    "stats.createOrReplaceTempView('class_stats')\n",
    "print(f'class_stats: {stats.count()} rows')\n",
    "\n",
    "# Curated tracking (partitioned by lab_id)\n",
    "tracking = spark.read.parquet(os.path.join(CURATED, 'tracking_curated'))\n",
    "tracking.createOrReplaceTempView('tracking')\n",
    "print(f'tracking: {tracking.count():,} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lab Distribution (SparkSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_dist = spark.sql(\"\"\"\n",
    "    SELECT lab_id, COUNT(*) AS video_count\n",
    "    FROM metadata_train\n",
    "    GROUP BY lab_id\n",
    "    ORDER BY video_count DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.barplot(data=lab_dist, y='lab_id', x='video_count', ax=ax, color='steelblue')\n",
    "ax.set_title('Train: Number of Videos per Lab')\n",
    "ax.set_xlabel('Number of Videos')\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Video Metadata Summary (SparkSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_summary = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        split,\n",
    "        COUNT(*)                              AS n_videos,\n",
    "        COUNT(DISTINCT lab_id)                AS n_labs,\n",
    "        ROUND(AVG(video_duration_sec), 1)     AS avg_duration_sec,\n",
    "        ROUND(AVG(frames_per_second), 1)      AS avg_fps,\n",
    "        ROUND(AVG(n_mice), 2)                 AS avg_mice\n",
    "    FROM (\n",
    "        SELECT * FROM metadata_train\n",
    "        UNION ALL\n",
    "        SELECT * FROM metadata_test\n",
    "    )\n",
    "    GROUP BY split\n",
    "    ORDER BY split\n",
    "\"\"\").toPandas()\n",
    "\n",
    "display(meta_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_df = spark.sql(\"\"\"\n",
    "    SELECT video_duration_sec FROM metadata_train\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fps_df = spark.sql(\"\"\"\n",
    "    SELECT frames_per_second, COUNT(*) AS cnt\n",
    "    FROM metadata_train\n",
    "    GROUP BY frames_per_second\n",
    "    ORDER BY frames_per_second\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.histplot(duration_df['video_duration_sec'], bins=50, ax=axes[0], color='teal', edgecolor='white')\n",
    "axes[0].set_title('Video Duration Distribution')\n",
    "axes[0].set_xlabel('Seconds')\n",
    "\n",
    "sns.barplot(data=fps_df, x='frames_per_second', y='cnt', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Frame Rate (FPS) Distribution')\n",
    "axes[1].set_ylabel('Number of Videos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Behavior Class Distribution — Extreme Imbalance (SparkSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        action,\n",
    "        SUM(event_count)   AS total_events,\n",
    "        SUM(total_frames)  AS total_frames\n",
    "    FROM class_stats\n",
    "    GROUP BY action\n",
    "    ORDER BY total_events DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.barplot(data=class_dist.sort_values('total_events'),\n",
    "            y='action', x='total_events', ax=axes[0], color='darkorange')\n",
    "axes[0].set_title('Annotation Events per Action')\n",
    "axes[0].set_xlabel('Event Count')\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "sns.barplot(data=class_dist.sort_values('total_frames'),\n",
    "            y='action', x='total_frames', ax=axes[1], color='seagreen')\n",
    "axes[1].set_title('Total Annotated Frames per Action')\n",
    "axes[1].set_xlabel('Frame Count')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Imbalance ratio (max/min events): '\n",
    "      f'{class_dist[\"total_events\"].max() / class_dist[\"total_events\"].min():.0f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Event Duration Analysis (SparkSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_stats = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        action,\n",
    "        COUNT(*)                                        AS n_events,\n",
    "        ROUND(AVG(duration_frames), 1)                  AS avg_dur,\n",
    "        PERCENTILE_APPROX(duration_frames, 0.5)         AS median_dur,\n",
    "        MIN(duration_frames)                             AS min_dur,\n",
    "        MAX(duration_frames)                             AS max_dur\n",
    "    FROM annotations\n",
    "    GROUP BY action\n",
    "    ORDER BY median_dur DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.barplot(data=duration_stats.sort_values('median_dur'),\n",
    "            y='action', x='median_dur', ax=ax, color='indianred')\n",
    "ax.set_title('Median Event Duration per Action (frames)')\n",
    "ax.set_xlabel('Frames')\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lab × Action Heatmap (SparkSQL + Seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_action = spark.sql(\"\"\"\n",
    "    SELECT lab_id, action, SUM(event_count) AS events\n",
    "    FROM class_stats\n",
    "    GROUP BY lab_id, action\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pivot = lab_action.pivot(index='lab_id', columns='action', values='events').fillna(0)\n",
    "\n",
    "# Keep top 20 actions by total count\n",
    "top_actions = lab_action.groupby('action')['events'].sum().nlargest(20).index\n",
    "pivot_top = pivot[[c for c in top_actions if c in pivot.columns]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.heatmap(np.log1p(pivot_top), ax=ax, cmap='YlOrRd', linewidths=0.5,\n",
    "            cbar_kws={'label': 'log(count+1)'}, xticklabels=True, yticklabels=True)\n",
    "ax.set_title('Lab × Action Event Count Heatmap (log scale)')\n",
    "ax.set_xlabel('Action')\n",
    "ax.set_ylabel('Lab')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self vs Interaction Behaviors (SparkSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_stats = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        action_type,\n",
    "        SUM(event_count)  AS total_events,\n",
    "        SUM(total_frames) AS total_frames\n",
    "    FROM class_stats\n",
    "    GROUP BY action_type\n",
    "    ORDER BY action_type\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pal = sns.color_palette(['#66c2a5', '#fc8d62'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.barplot(data=type_stats, x='action_type', y='total_events', ax=axes[0], palette=pal)\n",
    "axes[0].set_title('Event Count: Self vs Interaction')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('Number of Events')\n",
    "for p in axes[0].patches:\n",
    "    axes[0].annotate(f'{int(p.get_height()):,}',\n",
    "                     (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                     ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "sns.barplot(data=type_stats, x='action_type', y='total_frames', ax=axes[1], palette=pal)\n",
    "axes[1].set_title('Total Frames: Self vs Interaction')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_ylabel('Total Frames')\n",
    "for p in axes[1].patches:\n",
    "    axes[1].annotate(f'{int(p.get_height()):,}',\n",
    "                     (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                     ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tracking Data Quality Audit (SparkSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        lab_id,\n",
    "        COUNT(*)                                                    AS total_rows,\n",
    "        ROUND(100.0 * SUM(CASE WHEN x_valid = 0 THEN 1 ELSE 0 END) / COUNT(*), 2) AS nan_x_pct,\n",
    "        ROUND(100.0 * SUM(CASE WHEN y_valid = 0 THEN 1 ELSE 0 END) / COUNT(*), 2) AS nan_y_pct,\n",
    "        ROUND(AVG(x_cm), 3)                                        AS avg_x_cm,\n",
    "        ROUND(AVG(y_cm), 3)                                        AS avg_y_cm\n",
    "    FROM tracking\n",
    "    GROUP BY lab_id\n",
    "    ORDER BY nan_x_pct DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.barplot(data=quality.sort_values('nan_x_pct', ascending=False),\n",
    "            y='lab_id', x='nan_x_pct', ax=ax, color='salmon')\n",
    "ax.set_title('Missing Coordinate Rate (%) per Lab')\n",
    "ax.set_xlabel('NaN x-coordinate %')\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train vs Test Comparison (SparkSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        split,\n",
    "        COUNT(*)                              AS n_videos,\n",
    "        COUNT(DISTINCT lab_id)                AS n_labs,\n",
    "        ROUND(AVG(video_duration_sec), 1)     AS avg_dur_sec,\n",
    "        ROUND(PERCENTILE_APPROX(video_duration_sec, 0.5), 1) AS median_dur_sec,\n",
    "        ROUND(AVG(n_mice), 2)                 AS avg_mice,\n",
    "        COUNT(DISTINCT body_parts_tracked)    AS n_tracking_schemes\n",
    "    FROM (\n",
    "        SELECT * FROM metadata_train\n",
    "        UNION ALL\n",
    "        SELECT * FROM metadata_test\n",
    "    )\n",
    "    GROUP BY split\n",
    "\"\"\").toPandas()\n",
    "\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "| Finding | Detail |\n",
    "|---|---|\n",
    "| **Severe class imbalance** | `sniff` dominates; `ejaculate` has only 3 events (~12,600x ratio) |\n",
    "| **High lab heterogeneity** | 19+ labs with different FPS, keypoint schemes, arena shapes |\n",
    "| **Wide event duration range** | From single frames to thousands of frames |\n",
    "| **Missing data varies by lab** | Some labs have >5% NaN in tracking coordinates |\n",
    "| **Interaction behaviors dominate** | Both in variety and total annotated frames |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print('SparkSession stopped. EDA complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
